{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Implementation\n",
    "-----------------------------\n",
    "## Learning Algorithm\n",
    "The Deep Deterministic Policy Gradient (DDPG) Algorithm detailed in [this research paper](https://arxiv.org/pdf/1509.02971.pdf) is used. The algorithm can be outlined as follows:\n",
    "- Randomly initialize critic network and actor.\n",
    "- Initialize target network.\n",
    "- Initialize replay buffer `R`.\n",
    "- **for** episode = 1 to `M` **do**:\n",
    "    - Initialize a random process for action exploration.\n",
    "    - Receive initial observation states.\n",
    "    - **for** t = 1 to `T` **do**:\n",
    "        - Select action according to the current policy and exploration noise.\n",
    "        - Execute action and observe reward and observe new state.\n",
    "        - Store transition `R`.\n",
    "        - Sample a random minibatch of `N` transitions from `R`.\n",
    "        - Update critic by minimizing the loss.\n",
    "        - Update the actor policy using the sampled policy gradient.\n",
    "        - Update the target networks.\n",
    "        \n",
    "        **end for** \n",
    "    \n",
    "  **end for**\n",
    "  \n",
    "## Hyperparameters\n",
    "The hyperparameters used were from the previous lesson, which were not very effective. Increasing the batch size and decreasing the discount factor only made a slight difference. What really helped stabilize the training was: first, training on the whole episode at a time and removing the limit on time steps (which makes sense because it makes the agent learn more effectively about the entire episode), and, second, minimizing the standard deviation (sigma) of the OU Noise. \n",
    "\n",
    "### Agent Hyperparameters\n",
    "```\n",
    "BUFFER_SIZE = int(1e6) # replay buffer size\n",
    "BATCH_SIZE = 256       # minibatch size\n",
    "GAMMA = 0.9            # discount factor\n",
    "TAU = 1e-3             # for soft update of target parameters\n",
    "LR_ACTOR = 1e-3        # learning rate of the actor\n",
    "LR_CRITIC = 1e-3       # learning rate of the critic\n",
    "WEIGHT_DECAY = 0.0     # L2 weight decay\n",
    "```\n",
    "\n",
    "### Ornstein-Uhlenbeck Noise Hyperparameters\n",
    "```\n",
    "mu = 0.0\n",
    "theta = 0.15\n",
    "sigma = 0.01\n",
    "```\n",
    "\n",
    "## Model Architecture\n",
    "There are two neural networks in the DDPG algorithm.\n",
    "\n",
    "The **Actor Network Class** has one internal fully-connected layer with a depth of 128  which gave good result\n",
    "\n",
    "The **Critic Network Class** has three internal fully-connected layers of depth 256, 256 and 128 with batch normalization. Leaky relu activation is applied during forward pass to each of the hidden layers.\n",
    "\n",
    "One thing that made a significant improvement was the gradient clipping to update the network.\n",
    "\n",
    "## Visualization\n",
    "### Reward Plot\n",
    "<img src="https://user-images.githubusercontent.com/11984043/69648414-4a3c7580-1091-11ea-9ad1-16d0671239ef.PNG">\n",
    "\n",
    "### Total Number of Episodes\n",
    "```\n",
    "Episode 100\t    Average Score: 5.16\n",
    "Episode 200\t    Average Score: 28.50\n",
    "Episode 206\t    Average Score: 30.09\n",
    " Environment solved in 106 episodes!\tAverage Score: 30.09\n",
    "```\n",
    "\n",
    "## Ideas for Future Work\n",
    "\n",
    "Some more challenges to try would be to:\n",
    "- Add batch normalization to stabilize the training even more.\n",
    "- Playing around with the noise hyperparameters.\n",
    "- Even though I got good results considering I'm using only one agent, I think the DDPG algorithm is very unstable when it comes to hyperparameters. It needs optimum values to give good results. So, I would try implementing other algorithms using the same environment, such as [PPO](https://arxiv.org/pdf/1707.06347.pdf), [A3C](https://arxiv.org/pdf/1602.01783.pdf), and [D4PG](https://openreview.net/pdf?id=SyZipzbCb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:drlnd]",
   "language": "python",
   "name": "conda-env-drlnd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
